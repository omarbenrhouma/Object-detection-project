\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\geometry{margin=2.5cm}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4
}

\title{
    \vspace{-2cm}
    \textbf{Système de Détection d'Objets en Temps Réel\\
    utilisant YOLO et l'Accélération GPU}\\[0.5cm]
    \large{Détection de Casques de Sécurité et Équipements de Protection Individuelle (EPI)}
}
\author{
    Omar Mejri\\
    \textit{Projet de Vision par Ordinateur}
}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}

\subsection{Contexte et Objectifs}

Ce projet vise à développer un système de détection d'objets en temps réel pour la sécurité en milieu de travail, spécifiquement pour la détection des casques de sécurité (hard hats) et des équipements de protection individuelle (EPI). Le système utilise l'architecture YOLO (You Only Look Once) version 8, optimisée pour l'accélération GPU via CUDA.

Les objectifs principaux du projet sont les suivants :

\begin{itemize}
    \item Développer un modèle de détection capable d'identifier trois classes : tête (head), casque (helmet), et personne (person)
    \item Optimiser le modèle pour l'exécution en temps réel avec accélération GPU
    \item Évaluer les performances du modèle sur un ensemble de données de test
    \item Documenter l'ensemble du processus de développement, de la collecte de données à l'évaluation finale
\end{itemize}

\subsection{Importance de l'Accélération GPU}

L'accélération GPU est essentielle pour ce projet car elle permet :

\begin{itemize}
    \item \textbf{Inference en temps réel} : Traitement de plusieurs images par seconde (20-50 FPS)
    \item \textbf{Entraînement accéléré} : Réduction significative du temps d'entraînement
    \item \textbf{Optimisation mémoire} : Utilisation efficace de la mémoire GPU avec la précision mixte (FP16)
    \item \textbf{Évolutivité} : Capacité à traiter des vidéos haute résolution en temps réel
\end{itemize}

\section{Configuration de l'Environnement GPU}

\subsection{Installation de CUDA}

Pour utiliser l'accélération GPU, nous avons installé CUDA (Compute Unified Device Architecture), une plateforme de calcul parallèle développée par NVIDIA.

\subsubsection{Étapes d'Installation}

\begin{enumerate}
    \item \textbf{Téléchargement de CUDA Toolkit 12.6} : Téléchargé depuis le site officiel de NVIDIA
    \item \textbf{Installation} : Installation standard avec les paramètres par défaut
    \item \textbf{Configuration des variables d'environnement} : Ajout du chemin CUDA au PATH système
    \begin{lstlisting}
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin
    \end{lstlisting}
    \item \textbf{Vérification} : Utilisation de la commande \texttt{nvcc --version} pour vérifier l'installation
\end{enumerate}

\textbf{Note pour les captures d'écran} : Veuillez inclure une capture d'écran montrant la sortie de la commande \texttt{nvcc --version} confirmant l'installation de CUDA 12.6.

\subsection{Installation de cuDNN}

cuDNN (CUDA Deep Neural Network library) est une bibliothèque optimisée pour les opérations de deep learning sur GPU.

\begin{itemize}
    \item Téléchargement de cuDNN depuis le portail développeur NVIDIA
    \item Extraction et copie des fichiers dans le répertoire CUDA
    \item Vérification de l'installation via les tests de performance
\end{itemize}

\subsection{Installation de PyTorch avec Support CUDA}

PyTorch a été installé avec le support CUDA 12.6 :

\begin{lstlisting}
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
\end{lstlisting}

\subsection{Vérification de la Configuration GPU}

La vérification de la configuration GPU a été effectuée avec le code suivant :

\begin{lstlisting}
import torch
print("CUDA disponible:", torch.cuda.is_available())
print("Nom du GPU:", torch.cuda.get_device_name(0))
print("Version CUDA (PyTorch):", torch.version.cuda)
print("Mémoire GPU:", torch.cuda.get_device_properties(0).total_memory / 1e9, "GB")
\end{lstlisting}

\textbf{Résultats} :
\begin{itemize}
    \item GPU : NVIDIA GeForce RTX 3060 Laptop GPU
    \item Mémoire GPU : 6.44 GB
    \item Version CUDA : 12.6
    \item CUDA disponible : True
\end{itemize}

\textbf{Note pour les captures d'écran} : Veuillez inclure une capture d'écran montrant la sortie de cette vérification.

\section{Collecte et Préparation des Données}

\subsection{Source des Données : Roboflow}

Le dataset utilisé provient de Roboflow Universe, une plateforme collaborative pour les datasets de vision par ordinateur. Le dataset "Hard Hat Workers" a été sélectionné pour sa pertinence et sa qualité.

\subsubsection{Caractéristiques du Dataset}

\begin{itemize}
    \item \textbf{Source} : Roboflow Universe - Hard Hat Workers
    \item \textbf{Licence} : Public Domain
    \item \textbf{Classes} : 3 classes (head, helmet, person)
    \item \textbf{Format} : YOLO (annotations au format texte)
    \item \textbf{Répartition} : Train/Valid/Test (70/20/10)
\end{itemize}

\subsubsection{Structure du Dataset}

Le dataset est organisé selon la structure YOLO standard :

\begin{lstlisting}
data/
├── data.yaml
├── train/
│   ├── images/
│   └── labels/
├── valid/
│   ├── images/
│   └── labels/
└── test/
    ├── images/
    └── labels/
\end{lstlisting}

\subsection{Configuration du Dataset (data.yaml)}

Le fichier \texttt{data.yaml} contient la configuration du dataset :

\begin{lstlisting}
train: train/images
val: valid/images
test: test/images

nc: 3
names: ['head', 'helmet', 'person']
\end{lstlisting}

\subsection{Statistiques du Dataset}

\textbf{Note pour les tableaux/figures} : Veuillez inclure un tableau récapitulatif montrant :
\begin{itemize}
    \item Nombre total d'images par split (train/valid/test)
    \item Nombre total de labels
    \item Distribution des classes
    \item Statistiques sur les résolutions d'images
\end{itemize}

\section{Analyse Exploratoire des Données (EDA)}

\subsection{Validation de la Structure du Dataset}

La première étape de l'EDA consiste à valider la structure du dataset et vérifier l'intégrité des paires image-label.

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Un graphique montrant la distribution des images par split
    \item Un graphique montrant la distribution des classes (head, helmet, person)
    \item Un tableau montrant les statistiques de correspondance image-label
\end{itemize}

\subsection{Analyse des Résolutions d'Images}

L'analyse des résolutions d'images permet de comprendre la diversité du dataset et d'optimiser les paramètres d'entraînement.

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Histogramme des largeurs d'images
    \item Histogramme des hauteurs d'images
    \item Graphique de distribution des ratios d'aspect
    \item Tableau des résolutions les plus communes
\end{itemize}

\subsection{Analyse des Boîtes de Détection}

L'analyse des boîtes de détection (bounding boxes) fournit des informations cruciales sur la distribution des objets dans les images.

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Histogramme des largeurs normalisées des boîtes
    \item Histogramme des hauteurs normalisées des boîtes
    \item Histogramme des aires normalisées
    \item Distribution des ratios d'aspect par classe
    \item Catégorisation des boîtes (petites/moyennes/grandes)
\end{itemize}

\subsection{Visualisation d'Exemples}

La visualisation d'exemples permet de comprendre la qualité et la diversité des annotations.

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item 4-6 exemples d'images avec leurs annotations (une par split)
    \item Les boîtes doivent être colorées différemment selon la classe :
    \begin{itemize}
        \item Rouge pour "head"
        \item Vert (lime) pour "helmet"
        \item Cyan pour "person"
    \end{itemize}
\end{itemize}

\subsection{Distribution des Classes}

L'analyse de la distribution des classes est essentielle pour identifier d'éventuels déséquilibres.

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Graphique en barres montrant le nombre d'instances par classe
    \item Graphique en barres groupées montrant la distribution par split
    \item Tableau croisé classe/split
\end{itemize}

\section{Architecture YOLO}

\subsection{Présentation de YOLO}

YOLO (You Only Look Once) est une architecture de détection d'objets qui traite l'image entière en une seule passe, contrairement aux méthodes basées sur les régions qui nécessitent plusieurs passes.

\subsection{Avantages de YOLO}

\begin{itemize}
    \item \textbf{Vitesse} : Traitement en temps réel grâce à l'architecture unifiée
    \item \textbf{Précision} : Bonnes performances sur les objets de taille moyenne et grande
    \item \textbf{Simplicité} : Architecture end-to-end facile à entraîner
    \item \textbf{Efficacité GPU} : Optimisé pour l'exécution parallèle sur GPU
\end{itemize}

\subsection{Architecture YOLOv8}

YOLOv8 est la dernière version de la série YOLO, introduisant plusieurs améliorations :

\subsubsection{Composants Principaux}

\begin{enumerate}
    \item \textbf{Backbone (CSPDarknet)} : Extrait les caractéristiques de l'image
    \begin{itemize}
        \item Utilise des blocs CSP (Cross Stage Partial)
        \item Architecture en forme de pyramide pour capturer des caractéristiques multi-échelles
    \end{itemize}
    
    \item \textbf{Neck (PANet)} : Combine les caractéristiques de différentes échelles
    \begin{itemize}
        \item Path Aggregation Network (PAN)
        \item Fusion des caractéristiques de bas et haut niveau
    \end{itemize}
    
    \item \textbf{Head (Détection)} : Prédit les boîtes et classes
    \begin{itemize}
        \item Découplage des prédictions de classe et de localisation
        \item Utilisation de l'Anchor-Free pour plus de simplicité
    \end{itemize}
\end{enumerate}

\subsection{Format de Sortie YOLO}

Pour chaque objet détecté, YOLO prédit :
\begin{itemize}
    \item \textbf{Coordonnées de la boîte} : $(x, y, w, h)$ normalisées (centre, largeur, hauteur)
    \item \textbf{Score de confiance} : Probabilité qu'un objet soit présent
    \item \textbf{Probabilités de classe} : Probabilité pour chaque classe
\end{itemize}

\subsection{Optimisation GPU}

YOLOv8 est optimisé pour GPU grâce à :
\begin{itemize}
    \item \textbf{Opérations parallèles} : Convolutions et opérations matricielles parallélisées
    \item \textbf{Précision mixte} : Support FP16 pour doubler la vitesse
    \item \textbf{Optimisation TensorRT} : Possibilité d'optimisation supplémentaire
    \item \textbf{Batch processing} : Traitement de plusieurs images simultanément
\end{itemize}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Un schéma de l'architecture YOLOv8
    \item Un diagramme montrant le flux de données
    \item Un graphique comparant les performances CPU vs GPU
\end{itemize}

\section{Entraînement du Modèle}

\subsection{Configuration de l'Entraînement}

\subsubsection{Modèle de Base}

Nous avons utilisé YOLOv8s (small) comme modèle de base, offrant un bon compromis entre vitesse et précision.

\subsubsection{Stratégie d'Entraînement en Deux Phases}

Pour optimiser l'entraînement et éviter le surapprentissage, nous avons adopté une stratégie en deux phases :

\textbf{Phase 1 : Entraînement avec Backbone Gelé (15 époques)}
\begin{itemize}
    \item Gel des 10 premières couches du backbone
    \item Taux d'apprentissage initial : 0.01
    \item Augmentations fortes pour la généralisation
    \item Objectif : Initialiser les poids de manière stable
\end{itemize}

\textbf{Phase 2 : Fine-tuning Complet (30 époques)}
\begin{itemize}
    \item Toutes les couches sont entraînables
    \item Taux d'apprentissage réduit : 0.001
    \item Augmentations modérées
    \item Objectif : Affiner les détails et améliorer la précision
\end{itemize}

\subsection{Hyperparamètres}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Paramètre} & \textbf{Phase 1} & \textbf{Phase 2} \\
\hline
Taille d'image & 640x640 & 640x640 \\
Batch size & 8 & 8 \\
Taux d'apprentissage initial & 0.01 & 0.001 \\
Taux d'apprentissage final & 0.01 & 0.01 \\
Weight decay & 0.0005 & 0.0005 \\
Momentum & 0.937 & 0.937 \\
Patience & 20 & 25 \\
\hline
\end{tabular}
\caption{Hyperparamètres d'entraînement}
\end{table}

\subsection{Augmentations de Données}

Les augmentations de données sont cruciales pour améliorer la généralisation du modèle.

\subsubsection{Augmentations Spatiales}

\begin{itemize}
    \item \textbf{Rotation} : ±10° (Phase 1), ±5° (Phase 2)
    \item \textbf{Translation} : 10\% (Phase 1), 5\% (Phase 2)
    \item \textbf{Mise à l'échelle} : 0.5-1.5 (Phase 1), 0.7-1.3 (Phase 2)
    \item \textbf{Cisaillement} : ±2° (Phase 1), ±1° (Phase 2)
    \item \textbf{Perspective} : Légère transformation perspective
    \item \textbf{Retournement horizontal} : 50\% de probabilité
    \item \textbf{Retournement vertical} : 0\% (orientation importante pour les personnes)
\end{itemize}

\subsubsection{Augmentations de Couleur}

\begin{itemize}
    \item \textbf{HSV} : Variation de teinte, saturation et valeur
    \item \textbf{Mixup} : Mélange de deux images
    \item \textbf{Copy-Paste} : Copie d'objets entre images
\end{itemize}

\subsubsection{Augmentations Avancées}

\begin{itemize}
    \item \textbf{Mosaic} : Combinaison de 4 images en une
    \item \textbf{Probabilité} : 100\% (Phase 1), 80\% (Phase 2)
    \item \textbf{Close Mosaic} : Désactivation progressive en fin d'entraînement
\end{itemize}

\subsection{Optimisation GPU}

\subsubsection{Précision Mixte (AMP)}

L'Automatic Mixed Precision (AMP) permet d'utiliser FP16 pour la plupart des opérations, réduisant la mémoire et doublant la vitesse :

\begin{lstlisting}
amp=True  # Active la précision mixte
\end{lstlisting}

\subsubsection{Ajustement de la Taille de Batch}

La taille de batch est ajustée automatiquement selon la mémoire GPU disponible :

\begin{itemize}
    \item GPU ≥ 12GB : Batch size = 16
    \item GPU ≥ 8GB : Batch size = 12
    \item GPU ≥ 6GB (RTX 3060) : Batch size = 8
    \item GPU < 6GB : Batch size = 4
\end{itemize}

\subsubsection{Workers pour le Chargement des Données}

Sur Windows, nous utilisons \texttt{workers=0} pour éviter les problèmes de mémoire avec le multiprocessing.

\subsection{Courbes d'Entraînement}

\textbf{Note pour les figures} : Veuillez inclure les courbes d'entraînement montrant :
\begin{itemize}
    \item Perte totale (train et validation) vs époques
    \item Perte de classification vs époques
    \item Perte de localisation (box) vs époques
    \item mAP@0.5 vs époques
    \item mAP@0.5:0.95 vs époques
    \item Précision vs époques
    \item Rappel vs époques
\end{itemize}

\subsection{Résultats de l'Entraînement}

\textbf{Note pour les tableaux} : Veuillez inclure un tableau récapitulatif des métriques finales :
\begin{itemize}
    \item mAP@0.5
    \item mAP@0.5:0.95
    \item Précision
    \item Rappel
    \item F1-score
    \item Par classe (head, helmet, person)
\end{itemize}

\section{Évaluation et Résultats}

\subsection{Métriques d'Évaluation}

Les métriques utilisées pour évaluer le modèle sont standard dans la détection d'objets :

\subsubsection{mAP (mean Average Precision)}

\begin{itemize}
    \item \textbf{mAP@0.5} : Précision moyenne avec IoU seuil à 0.5
    \item \textbf{mAP@0.5:0.95} : Précision moyenne avec IoU de 0.5 à 0.95 (pas de 0.05)
\end{itemize}

\subsubsection{Précision et Rappel}

\begin{itemize}
    \item \textbf{Précision} : Proportion de détections correctes parmi toutes les détections
    \item \textbf{Rappel} : Proportion d'objets réels détectés
    \item \textbf{F1-score} : Moyenne harmonique de précision et rappel
\end{itemize}

\subsection{Matrice de Confusion}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Matrice de confusion normalisée
    \item Matrice de confusion non-normalisée
    \item Analyse des erreurs de classification
\end{itemize}

\subsection{Résultats sur l'Ensemble de Test}

\textbf{Note pour les tableaux} : Veuillez inclure un tableau détaillé des résultats par classe :
\begin{itemize}
    \item Nombre d'images testées
    \item Nombre total de détections
    \item Distribution des détections par classe
    \item Taux de détection moyen par image
\end{itemize}

\subsection{Exemples de Prédictions}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item 6-8 exemples de prédictions sur images de test
    \item Mélange de cas : détections correctes, faux positifs, faux négatifs
    \item Images avec différentes conditions (éclairage, angle, etc.)
\end{itemize}

\section{Inference en Temps Réel}

\subsection{Optimisation pour l'Inference GPU}

L'inference en temps réel nécessite plusieurs optimisations :

\subsubsection{Précision FP16}

L'utilisation de la précision demi (FP16) permet de :
\begin{itemize}
    \item Réduire la mémoire GPU de moitié
    \item Doubler la vitesse d'inference
    \item Maintenir une précision acceptable
\end{itemize}

\subsubsection{Traitement par Lots}

Le traitement par lots permet de traiter plusieurs images simultanément, améliorant le débit.

\subsubsection{Optimisation du Pipeline}

\begin{itemize}
    \item Préprocessing optimisé sur GPU
    \item Inference asynchrone
    \item Postprocessing efficace
\end{itemize}

\subsection{Inference sur Images}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Exemples de prédictions sur images individuelles
    \item Temps d'inference par image
    \item Utilisation de la mémoire GPU
\end{itemize}

\subsection{Inference sur Vidéo}

Pour l'inference vidéo en temps réel, nous avons implémenté :

\begin{itemize}
    \item \textbf{Chargement vidéo} : Utilisation d'OpenCV pour la lecture vidéo
    \item \textbf{Traitement frame par frame} : Chaque frame est traitée sur GPU
    \item \textbf{Affichage en temps réel} : Résultats affichés avec FPS en direct
    \item \textbf{Optimisation mémoire} : Nettoyage du cache GPU entre les frames
\end{itemize}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Capture d'écran de l'inference vidéo en cours
    \item Graphique montrant les FPS au fil du temps
    \item Comparaison FPS avec différentes configurations (CPU vs GPU, FP32 vs FP16)
\end{itemize}

\subsection{Performances}

\textbf{Résultats de Performance} :
\begin{itemize}
    \item \textbf{FPS moyen} : 25-35 FPS sur RTX 3060
    \item \textbf{Temps d'inference} : 20-40ms par frame
    \item \textbf{Utilisation GPU} : 60-80\% selon la résolution
    \item \textbf{Mémoire GPU} : ~2-3 GB utilisés
\end{itemize}

\section{Analyse de l'Accélération GPU}

\subsection{Comparaison CPU vs GPU}

\textbf{Note pour les tableaux/figures} : Veuillez inclure :
\begin{itemize}
    \item Tableau comparatif des temps d'inference CPU vs GPU
    \item Graphique comparatif des FPS
    \item Analyse de l'accélération obtenue
\end{itemize}

\subsection{Impact de la Précision Mixte}

L'utilisation de FP16 apporte des gains significatifs :

\begin{itemize}
    \item \textbf{Vitesse} : 1.8-2x plus rapide qu'FP32
    \item \textbf{Mémoire} : Réduction de 50\%
    \item \textbf{Précision} : Perte négligeable (< 1\%)
\end{itemize}

\subsection{Optimisation de la Taille de Batch}

\textbf{Note pour les figures} : Veuillez inclure :
\begin{itemize}
    \item Graphique montrant le débit (throughput) vs taille de batch
    \item Analyse de l'utilisation mémoire vs taille de batch
    \item Point optimal identifié
\end{itemize}

\section{Challenges et Solutions}

\subsection{Problèmes Rencontrés}

\subsubsection{Problèmes de Mémoire}

\textbf{Problème} : Erreurs de mémoire lors de l'entraînement avec multiprocessing sur Windows.

\textbf{Solution} : Utilisation de \texttt{workers=0} sur Windows pour éviter les problèmes de mémoire avec les processus multiples.

\subsubsection{Déséquilibre des Classes}

\textbf{Problème} : Distribution inégale des classes (plus de "helmet" que "head").

\textbf{Solution} : Utilisation d'augmentations ciblées et ajustement des poids de classe si nécessaire.

\subsubsection{Optimisation GPU}

\textbf{Problème} : Utilisation sous-optimale de la mémoire GPU.

\textbf{Solution} : Ajustement automatique de la taille de batch selon la mémoire disponible et utilisation de FP16.

\section{Conclusion}

\subsection{Résultats Principaux}

Ce projet a permis de développer un système de détection d'EPI fonctionnel avec les résultats suivants :

\begin{itemize}
    \item Modèle entraîné avec de bonnes performances sur les trois classes
    \item Inference en temps réel avec accélération GPU
    \item Pipeline complet de la collecte de données à l'évaluation
    \item Documentation complète du processus
\end{itemize}

\subsection{Contributions}

Les contributions principales de ce projet sont :

\begin{itemize}
    \item Adaptation d'un modèle YOLO pour la détection d'EPI
    \item Optimisation pour l'exécution GPU en temps réel
    \item Documentation complète du processus de développement
    \item Analyse approfondie des performances
\end{itemize}

\subsection{Perspectives Futures}

Les améliorations possibles incluent :

\begin{itemize}
    \item \textbf{Amélioration de la précision} : Collecte de plus de données, fine-tuning plus poussé
    \item \textbf{Optimisation supplémentaire} : Utilisation de TensorRT pour une accélération encore plus importante
    \item \textbf{Extension des classes} : Ajout d'autres types d'EPI (gants, chaussures de sécurité, etc.)
    \item \textbf{Déploiement} : Intégration dans un système de surveillance en temps réel
    \item \textbf{Multi-caméra} : Support pour plusieurs flux vidéo simultanés
\end{itemize}

\subsection{Leçons Apprises}

\begin{itemize}
    \item L'importance de la préparation des données et de l'EDA
    \item L'efficacité de l'accélération GPU pour l'entraînement et l'inference
    \item La nécessité d'une stratégie d'entraînement en plusieurs phases
    \item L'importance de l'optimisation mémoire pour les systèmes avec ressources limitées
\end{itemize}

\section{Références}

\begin{itemize}
    \item Redmon, J., \& Farhadi, A. (2018). YOLOv3: An Incremental Improvement. arXiv preprint arXiv:1804.02767.
    \item Ultralytics. (2023). YOLOv8 Documentation. \url{https://docs.ultralytics.com}
    \item Roboflow. (2023). Hard Hat Workers Dataset. \url{https://universe.roboflow.com}
    \item NVIDIA. (2023). CUDA Toolkit Documentation. \url{https://docs.nvidia.com/cuda}
    \item PyTorch. (2023). PyTorch Documentation. \url{https://pytorch.org/docs}
\end{itemize}

\section{Annexes}

\subsection{Annexe A : Code Source}

Le code source complet est disponible dans le dépôt GitHub du projet.

\subsection{Annexe B : Structure du Projet}

\begin{lstlisting}
object_detection_project/
├── data/
│   ├── data.yaml
│   ├── train/
│   ├── valid/
│   └── test/
├── notebooks/
│   ├── 01_data_collection.ipynb
│   ├── 02_eda.ipynb
│   ├── 03_train_ppe.ipynb
│   └── 04_predict_ppe.ipynb
├── src/
│   ├── train.py
│   ├── inference.py
│   └── utils.py
├── models/
└── runs/
\end{lstlisting}

\subsection{Annexe C : Commandes Utiles}

\begin{lstlisting}
# Vérification CUDA
nvcc --version

# Vérification PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Entraînement
python src/train.py --data data/data.yaml --epochs 30

# Inference
python src/inference.py --model runs/detect/best.pt --source image.jpg
\end{lstlisting}

\end{document}

